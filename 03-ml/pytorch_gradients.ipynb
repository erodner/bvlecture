{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654d3271",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../htwlogo.jpg\">\n",
    "\n",
    "# Gradients with pytorch\n",
    "\n",
    "**Author**: Dive into Deep Learning, adapted by _Erik Rodner_<br>\n",
    "**Lecture**: Computer Vision and Machine Learning I\n",
    "\n",
    "In the following exercise, we will look at automatic differentiation.\n",
    "\n",
    "https://d2l.ai/chapter_preliminaries/autograd.html\n",
    "\n",
    "The following code requires pytorch and if you want to get the nice graph visualization you also **need graphviz being installed** (https://graphviz.org/download/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80692803",
   "metadata": {},
   "source": [
    "Let us first define a tensor that can store a corresponding gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c15f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(os.path.join(\"..\", \"utils\"))\n",
    "\n",
    "from torchutils import make_dot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  # Same as `x = torch.arange(4.0, requires_grad=True)`\n",
    "x.grad  # The default value is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b09e",
   "metadata": {},
   "source": [
    "``x`` is just a single node in the computation graph. So let's define a new variable ``y``\n",
    "which can be computed using ``x``, i.e. we mathematically we could write $y(x) = 2 x^T x = 2 \\sum\\limits_{d=1}^D x_i * x_i = 2 \\sum\\limits_{d=1}^D x_i^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ae435",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * torch.dot(x, x) # forward step\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b56d",
   "metadata": {},
   "source": [
    "Computing the gradient $\\nabla_x y = ( \\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}, \\frac{\\partial y}{\\partial x_3}, \\frac{\\partial y}{\\partial x_4} )$ is easy: \n",
    "1. backward step and then \n",
    "2. accessing ``x.grad``.\n",
    "\n",
    "Please note that this can only be done with ``x`` storing a proper value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe3c7f",
   "metadata": {},
   "source": [
    "Lets redefine $y(x)$ as $y(x) = \\sum_i x_i = x_1 + x_2 + x_3 + x_4$ and recompute the gradient: $\\nabla_x y = ( \\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}, \\frac{\\partial y}{\\partial x_3}, \\frac{\\partial y}{\\partial x_4} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3658ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "y = x.sum() # forward pass\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938ff0a",
   "metadata": {},
   "source": [
    "Now, let us move to an example with 3 tensors:\n",
    "1. ``z`` contains only 1s\n",
    "2. $y = x^T z = \\sum\\limits_i x_i * z_i = x_1*z_1 + x_2 * z_2 + x_3 * z_3 + x_4 * z_4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b09144",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones(4, requires_grad=True)\n",
    "y = torch.dot(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f253276",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e860f8a",
   "metadata": {},
   "source": [
    "After the backward operation, ``x.grad`` contains $\\nabla_x y$ and ``z.grad`` contains $\\nabla_z y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc1f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84003fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573e3c6",
   "metadata": {},
   "source": [
    "We can also visualize the computation graph and see which parts of the graph contain elements to store the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b55022",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(y, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b986c",
   "metadata": {},
   "source": [
    "We can also build more complex graphs like this one:\n",
    "1. $z_i = x_i^2 \\enspace \\forall i$\n",
    "2. $y = \\sum\\limits_i x_i * z_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x**2\n",
    "y = torch.dot(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c293a6d",
   "metadata": {},
   "source": [
    "Look how the variable z is now just an intermediate variable, which is not shown in the following graph. However, it is still part of the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840025dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(y, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "z.retain_grad() # this ensures that we can access the gradient, otherwise this would not be possible\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28635a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
