{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f71e918",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../htwlogo.jpg\">\n",
    "\n",
    "# Linear regression with pytorch\n",
    "\n",
    "**Author**: Dive into Deep Learning, adapted by _Erik Rodner_<br>\n",
    "**Lecture**: Computer Vision and Machine Learning I\n",
    "\n",
    "In the following exercise, we implement a simple linear regression method using pytorch - and all the definitions and optimizers that are common for deep learning - enjoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26481037",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(\"..\", \"utils\"))\n",
    "\n",
    "from vectorgen import synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b240d",
   "metadata": {},
   "source": [
    "### Generating synthetic data\n",
    "\n",
    "In the following, we generate and visualize some synthetic data in two dimensions that we use to define the regression task. \n",
    "Since this is simulated data, we exactly know the linear model parameters and can compare them with the ones\n",
    "the optimizer yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc56aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Shape of X (features): {features.shape}\")\n",
    "print (f\"Shape of y (labels): {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc533ca",
   "metadata": {},
   "source": [
    "Above code does not deal with numpy arrays as in previous notebooks. Instead, we use arrays defined in the (py)torch framework. They have the benefit of automatically being allocated on a GPU (if configured) and automatically stored in computational graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f2947",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "for i_dim in range(2):\n",
    "    plt.subplot(1,2,i_dim+1)\n",
    "    plt.scatter(features[:, i_dim].numpy(),\n",
    "                labels.numpy(), 2)\n",
    "    plt.ylabel(\"target\")\n",
    "    plt.xlabel(f\"dimension {i_dim}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f576a",
   "metadata": {},
   "source": [
    "### Defining the data iterator for later SGD\n",
    "\n",
    "Stochastic gradient descent requires taking out multiple examples from the training set as a batch. This is implemented\n",
    "in a data iterator. The data iterator decomposes the training set into batches of ``batch_size`` random examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09796613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    \n",
    "    # The examples are read at random, in no particular order,\n",
    "    # therefore, let us shuffle some indices\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # now we can output multiple random batches, ensuring\n",
    "    # that a single examples is only present in one batch during a single epoch\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # the min command ensures that we do not get into trouble at the end of \n",
    "        # the epoch\n",
    "        batch_indices = torch.tensor(indices[i:min(i +\n",
    "                                                   batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd006ff",
   "metadata": {},
   "source": [
    "If we apply the data iterator in a for loop and outputs batches for whole dataset. Looping once over the whole training set is \n",
    "referred to as ``epoch``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break # we have enough :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e31413",
   "metadata": {},
   "source": [
    "### Defining the model and its parameters\n",
    "\n",
    "Ok, data iterator/loader is ready. Now it is time to define the following:\n",
    "1. Initial values of the parameters $w$ and $b$\n",
    "2. The linear model ``linreg``: $\\hat{y}(X; w,b) = X*w + b$ - note that this definition allows multiple examples as input - $X$ is a matrix\n",
    "3. The loss function ``squared_loss``: $\\|\\hat{y}-y\\|^2$\n",
    "4. How the SGD optimization should work: ``sgd``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  \n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y): \n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # loop through all parameters and\n",
    "        # perform a single update step and zero the gradients afterwards\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf643fb1",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Let us start the optimization by running SGD for some epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f624fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 10\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # forward pass\n",
    "        l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`\n",
    "        # backward pass\n",
    "        # Compute gradient on `l` with respect to [`w`, `b`]\n",
    "        l.sum().backward()\n",
    "        # Update parameters using their gradient\n",
    "        sgd([w, b], lr, batch_size)  \n",
    "        \n",
    "    # simply output some loss function statistics by performing\n",
    "    # a forward pass again WITHOUT backward pass\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9bb31",
   "metadata": {},
   "source": [
    "### Compare estimated model parameters with the ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'error in estimating b: {true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a800e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
