{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338294c3",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../bvlecture_exercises/htwlogo.jpg\">\n",
    "\n",
    "# Exercise: spam classification\n",
    "\n",
    "**Author**: _Erik Rodner_<br>\n",
    "**Lecture**: Computer Vision and Machine Learning I\n",
    "\n",
    "In the following exercise, you need to implement your first machine learning system: classical spam classification.\n",
    "We will use a dataset of SMS messages that have been tagged as either ``spam`` or ``ham`` (normal message).\n",
    "The dataset was used in the kaggle challenge https://www.kaggle.com/uciml/sms-spam-collection-dataset and is originally from http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from os.path import isfile\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import numpy as np\n",
    "from scipy.sparse import find\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51e1ef",
   "metadata": {},
   "source": [
    "### Download and show the dataset\n",
    "\n",
    "The following code snipplet downloads the zip file and uncompresses it, such that you have the dataset on your jupyter notebook machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ce626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "if not isfile(\"SMSSpamCollection.txt\"):\n",
    "    print (\"Downloading dataset...\")\n",
    "    dataset_url = \"https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip\"\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    with urlopen(dataset_url, context=ctx) as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            zfile.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021cfc9d",
   "metadata": {},
   "source": [
    "Pandas is a great tool for reading and managing tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f469ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SMSSpamCollection.txt\", delimiter=\"\\t\", names=[\"label\", \"sentence\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9c81a",
   "metadata": {},
   "source": [
    "### Splitting and inspecting the dataset\n",
    "\n",
    "We need to split the dataset into a training set and a test set.\n",
    "``scikit-learn`` provides multiple tools for this very first task in machine learning. The following line uses stratified sampling - can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(df.index, train_size=0.01, stratify=df[\"label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42764905",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,3,1)\n",
    "df[\"label\"].hist()\n",
    "plt.title(\"all data\")\n",
    "plt.subplot(1,3,2)\n",
    "df[\"label\"][train_indices].hist()\n",
    "plt.title(\"training data\")\n",
    "plt.subplot(1,3,3)\n",
    "df[\"label\"][test_indices].hist()\n",
    "plt.title(\"test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e6c8f",
   "metadata": {},
   "source": [
    "### Converting text to vectors\n",
    "\n",
    "One way to deal with text is to convert it into proper vectors, but how?\n",
    "We use a bag-of-words feature extractor in the following. First it generates a vocabulary $V = \\{\\text{word}_1, \\text{word}_1, ..., \\text{word}_n \\}$ based on all training sentences (all words but without so called stop-words). Then, all words are counted in each sentence individually. A single feature $x_k$ in an example $\\mathbf{x}$ is then defined as the occurrences of the word $\\text{word}_k$ in the sentence. Can you spot any problems here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbdc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_features=10)\n",
    "\n",
    "y_train = df[\"label\"][train_indices].array\n",
    "X_train_counts = count_vect.fit_transform(df[\"sentence\"][train_indices])\n",
    "\n",
    "feature_dim = X_train_counts.shape[1]\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315eea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3780d",
   "metadata": {},
   "source": [
    "We remember which index corresponds to which word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8829da",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {v: k for k, v in count_vect.vocabulary_.items()}\n",
    "index_to_word[feature_dim//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_example(x, title=\"\"):\n",
    "    \"\"\" Helper function for visualizing bag-of-word vectors \"\"\"\n",
    "    _, x_indices, x_values = find(x)\n",
    "    words = \",\".join( [index_to_word[i] for i in x_indices ])\n",
    "    if len(words)>20:\n",
    "        words = words[:20] + \"...\"\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.bar(x_indices, height=x_values, width=20.0)\n",
    "    plt.xlabel(\"indices of the vector, word index in vocabulary\")\n",
    "    plt.title(title + \"; \" + words)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f16ce",
   "metadata": {},
   "source": [
    "Let's inspect some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79193f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 5, 7]:\n",
    "    visualize_example(X_train_counts[i,:], y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542e674",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce458d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.LinearSVC()\n",
    "np.random.shuffle(y_train)\n",
    "classifier.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56da6e4",
   "metadata": {},
   "source": [
    "### Inspect the classifier\n",
    "\n",
    "Show all components of $w$ in a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_example(classifier.coef_, \"classifier weights\")\n",
    "_, most_important_coef = np.where(np.abs(classifier.coef_)>0.5)\n",
    "important_words = [index_to_word[i] for i in most_important_coef ]\n",
    "print (\"\\n\".join(important_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e8c92",
   "metadata": {},
   "source": [
    "### Test and evaluate the classifier\n",
    "\n",
    "Now let's transform the test data and predict the classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6508e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(df[\"sentence\"][test_indices])\n",
    "y_test = df[\"label\"][test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = classifier.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49e51c",
   "metadata": {},
   "source": [
    "### Visualize the results\n",
    "\n",
    "A confusion matrix is a good way to obtain an overview of the classification results.\n",
    "\n",
    "This is how it should look like:\n",
    "<img src=\"spam_solution.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=C, display_labels=classifier.classes_)\n",
    "disp.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd61590",
   "metadata": {},
   "source": [
    "## What is the accuracy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3576511",
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a14883",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(np.diag(C))/np.sum(C)\n",
    "print (f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_ham_in_test = np.sum(y_test == \"ham\")\n",
    "total_examples = np.sum(C)\n",
    "print (f\"Accuracy of a trivial classifier: {number_of_ham_in_test/total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = np.mean(np.diag(C) / np.sum(C, axis=1))\n",
    "print (f\"Mean accuracy: {mean_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.decision_function(X_test_counts), y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2b6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
