{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccca0b80",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../bvlecture_exercises/htwlogo.jpg\">\n",
    "\n",
    "# Exercise: Where is Waldo ?\n",
    "\n",
    "**Author**: _Erik Rodner_ <br>\n",
    "**Lecture**: Computer Vision and Machine Learning I\n",
    "\n",
    "In this exercise, we will design an awkward convolutional neural network purely for template matching, i.e. finding a structure in an image by sliding-over it.\n",
    "\n",
    "**Can you spot the errors in architecture, the optimization, or the evaluation code?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some dependencies\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from skimage.feature import peak_local_max\n",
    "from collections import OrderedDict\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225e6a2",
   "metadata": {},
   "source": [
    "### Defining the simulation and the corresponding data iterator\n",
    "\n",
    "Our dataset will be generated in an synthetic manner:\n",
    "1. **Input image generation**:\n",
    "    1. A small icon is placed at a random location in a larger image\n",
    "    2. Salt-and-pepper noise is added \n",
    "2. **Target image generation**:\n",
    "    1. A 2D-Gaussian is placed at the position of the icon with fixed size (done by convolving an impulse with a large Gaussian filter)\n",
    "    \n",
    "The 2D-Gaussian simulates a filter output with highest peak at the icon location - this is exactly what we want the model to output after training. The dataset consists of 128 example but is completely random and 128 is a completely arbitrary value that simply defines an epoch length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3882f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, icon_fn=\"waldo_small.jpg\"):\n",
    "        \"\"\" Initialization of the dataset and preparing it \"\"\"\n",
    "        super(MyIterableDataset).__init__()\n",
    "\n",
    "        img_waldo = Image.open(icon_fn)\n",
    "\n",
    "        # some preprocessing independent of the random images\n",
    "        # later on - no gradients required here\n",
    "        with torch.no_grad():\n",
    "            # transform the PIL image to a tensor \n",
    "            self.input_tensor = transforms.ToTensor()(img_waldo)\n",
    "            # add a black border around the icon - this defines the size of the large image\n",
    "            self.input_tensor = transforms.functional.pad(self.input_tensor, 32, fill=0)\n",
    "            # ... the icon is now placed in the middle\n",
    "            \n",
    "            # target image should have the same size as input image\n",
    "            self.target_tensor = torch.zeros((1,self.input_tensor.shape[1],self.input_tensor.shape[2]))\n",
    "            # add a single white pixel in the middle\n",
    "            self.target_tensor[:, self.target_tensor.shape[1]//2, self.target_tensor.shape[2]//2] = 1\n",
    "            # convolve with a Gaussian filter generating a 2D Gaussian in the middle\n",
    "            self.target_tensor = transforms.GaussianBlur(61, (10, 10))(self.target_tensor)\n",
    "            # normalize the target image\n",
    "            self.target_tensor = self.target_tensor / torch.max(self.target_tensor)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\" Return an arbitrary length that defines an epoch \"\"\"\n",
    "        return 128\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" return a single item of the dataset \"\"\"\n",
    "        \n",
    "        # sample some translation parameters\n",
    "        # we could also change the code here to allow for rotation and scaling\n",
    "        affine_params = transforms.RandomAffine((0,0)).get_params(degrees=(0,0), \n",
    "                                                              translate=(0.3,0.3), \n",
    "                                                              scale_ranges=(1, 1), \n",
    "                                                              shears=(0, 0), \n",
    "                                                              img_size=self.target_tensor.size())\n",
    "\n",
    "        # apply the same transformation to both image and target\n",
    "        transformed_input = transforms.functional.affine(self.input_tensor, *affine_params)\n",
    "        transformed_target = transforms.functional.affine(self.target_tensor, *affine_params)\n",
    "        \n",
    "        # add salt-and-pepper noise to the input image\n",
    "        random_mask = torch.rand_like(transformed_input)\n",
    "        transformed_input[random_mask<0.05] = 0\n",
    "        transformed_input[random_mask>0.95] = 1\n",
    "        \n",
    "        return transformed_input, transformed_target\n",
    "        \n",
    "        \n",
    "# the dataset iterator\n",
    "train_dataset = MyIterableDataset()\n",
    "# the dataset loader that returns the data in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56c511",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Let's visualize a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e301acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch\n",
    "input_batch, target_batch = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "grid_inputs = torchvision.utils.make_grid(nrow=8, tensor=input_batch)\n",
    "plt.imshow(np.transpose(grid_inputs, axes=(1,2,0)))\n",
    "plt.title(\"input images\");\n",
    "plt.figure(figsize=(20,10))\n",
    "grid_targets = torchvision.utils.make_grid(nrow=8, tensor=target_batch)\n",
    "plt.imshow(np.transpose(grid_targets, axes=(1,2,0)))\n",
    "plt.title(\"target images\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b131780",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We will use the simplest ConvNet possible: a single convolution layer and a rectified linear unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network as an pytorch module\n",
    "class Network(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        # defining the layers of the convnet by defining\n",
    "        # a list of transformations that will be applied later on\n",
    "        self.layers = [\n",
    "             (\"conv1\", nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(5,5), padding=2)),\n",
    "             (\"relu1\", nn.ReLU()) \n",
    "        ]\n",
    "        \n",
    "        # the transformation is a composition of all previous transformations\n",
    "        self.seq = torch.nn.Sequential( OrderedDict(self.layers) )\n",
    "        \n",
    "        \n",
    "    def forward(self, t): \n",
    "        # forward pass\n",
    "        return self.seq(t) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb56a81",
   "metadata": {},
   "source": [
    "### Optimization loop\n",
    "In contrast to previous notebooks we will use a pytorch optimizer that provides\n",
    "us with Adam - improved stochastic gradient descent with adaptive step sizes.\n",
    "```optimizer``` performs the gradient update for us, which we previously performed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "cnn_model = Network()\n",
    "# define the optimizer and its parameters\n",
    "\n",
    "optimizer = optim.Adam(lr=0.1, params=cnn_model.parameters())\n",
    "\n",
    "# training loop\n",
    "for epoch in range(5):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    # loop through all batches of the data\n",
    "    for batch in train_data_loader:\n",
    "        input_imgs, target_imgs = batch\n",
    "        pred_imgs = cnn_model(input_imgs) # get preds\n",
    "        \n",
    "        # as a loss we will use a quadratic loss function here \n",
    "        # however, this is likely not a good choice\n",
    "        loss = F.mse_loss(pred_imgs, target_imgs)\n",
    "        optimizer.zero_grad() # zero grads\n",
    "        loss.backward() # calculates gradients \n",
    "        optimizer.step() # update the weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    end_time = time.time() - start_time    \n",
    "    print(\"Epoch no.\",epoch+1 ,\"|total_loss: \", total_loss, \"| epoch_duration: \", round(end_time,2),\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bf1e2",
   "metadata": {},
   "source": [
    "### Testing on the training set\n",
    "\n",
    "We now evaluate our model on the training set ....wait, isn't this illegal?\n",
    "Since we randomly sample the dataset, it is not :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = next(iter(train_data_loader))\n",
    "pred_batch = cnn_model.forward(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b74320",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err = 0\n",
    "for i in range(target_batch.shape[0]):\n",
    "    with torch.no_grad():\n",
    "        target_peak = peak_local_max(np.array(target_batch[i,0,...]), num_peaks=1)\n",
    "        pred_peak = peak_local_max(np.array(pred_batch[i,0,...]), num_peaks=1)\n",
    "        \n",
    "    if pred_peak.shape != (1,2):\n",
    "        pred_peak = np.array([[0,0]])\n",
    "    euclid_err = np.linalg.norm(target_peak-pred_peak)\n",
    "    total_err += euclid_err\n",
    "    print (f\"Example no. {i}: euclidean distance between peaks is {euclid_err:.2f}px\")\n",
    "    \n",
    "total_err /= target_batch.shape[0]    \n",
    "print (f\"Average error is {total_err:.2f}px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a680fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "grid_inputs = torchvision.utils.make_grid(nrow=8, tensor=input_batch)\n",
    "plt.imshow(np.transpose(grid_inputs, axes=(1,2,0)))\n",
    "plt.title(\"input images\");\n",
    "plt.figure(figsize=(20,10))\n",
    "grid_targets = torchvision.utils.make_grid(nrow=8, tensor=target_batch)\n",
    "plt.imshow(np.transpose(grid_targets, axes=(1,2,0)))\n",
    "plt.title(\"target images\");\n",
    "plt.figure(figsize=(20,10))\n",
    "grid_preds = torchvision.utils.make_grid(nrow=8, tensor=pred_batch)\n",
    "plt.imshow(np.transpose(grid_preds, axes=(1,2,0)))\n",
    "plt.title(\"predictions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d56cdf8",
   "metadata": {},
   "source": [
    "### Visualizing the parameter of the first convolutional layer\n",
    "\n",
    "Let us visualize the learned filter parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "with torch.no_grad():\n",
    "    conv_parameters = np.array(cnn_model.seq[0].weight[0,0,:,:])\n",
    "plt.imshow(conv_parameters, cmap=plt.cm.gray)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9e16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
